serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: serviceaccount.adaptive-ml.com

# Secrets configuration
# You can either provide values directly (and the chart will create secrets)
# or reference existing secrets that you provision yourself
secrets:
  # Optional: Reference existing secrets instead of creating new ones
  # Control Plane existing secret (must contain keys: dbUsername, dbPassword, dbHost, dbName, cookiesSecret, oidcProviders)
  existingControlPlaneSecret: ""
  # Harmony existing secret (must contain keys: modelRegistryUrl, sharedDirectoryUrl)
  existingHarmonySecret: ""
  # Redis existing secret (must contain key: redisUrl)
  existingRedisSecret: ""
  # S3 bucket for model registry
  modelRegistryUrl: s3://bucket-name/model_registry
  # Use same bucket as above and can use a different prefix
  sharedDirectoryUrl: s3://bucket-name/shared
  # Postgres database connection configuration
  db:
    username: username
    password: password
    host: db_address:5432  # Host and port (e.g., db_address:5432 or db_address)
    database: db_name
  # Secret used to sign cookies. Must be the same on all servers of a cluster and >= 64 chars
  cookiesSecret: change-me-secret-db40431e-c2fd-48a6-acd6-854232c2ed94-01dd4d01-dr7b-4315   # Must be >= 64 chars

  auth:
    oidc:
      providers:
        # Name of your OpenId provider displayed in the ui
        - name: "Google"
          # Key of your provider, the callback url will be '<rootUrl>/api/v1/auth/login/<key>/callback'
          key: "google"

          issuer_url: "https://accounts.google.com"   # openid connect issuer url

          client_id: "replace_client_id"   # client id

          client_secret: "replace_client_secret"   # client_secret, optional

          scopes: ["email", "profile"]   # scopes required for auth, requires email and profile

          # true if your provider supports pkce (recommended)
          pkce: true

          # if true, user account will be created if it does not exist
          allow_sign_up: true

          # if true, user email must be verified to log in to the application (default is true).
          # in some identity providers, this is not supported and should be opted out.
          require_email_verified: true

auth:
  # One of [admin, read-only, inference, annotator]
  default_role: admin
  session:
    # Set the secure flag for the session cookie: they are only valid on https and localhost
    # Should be true in prod - (use false if the app is accessed through insecure http)
    secure: true
    expiration_seconds: 518400   # 6 days
  # List of email addresses for admins; overrides default_role when these users are created
  admins: []

containerRegistry: <aws_account_id>.dkr.ecr.<region>.amazonaws.com   # Add the Adaptive Registry you have been granted access to

harmony:
  # Set to false to disable the harmony statefulset deployment (advanced mode only)
  enabled: true
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: harmony:latest   # Add the harmony image tag
    pullPolicy: Always

  group: default   # Visible name of the group for this statefulset
  partitionKey: default-statefulset   # Partition name, should be unique across other groups

  replicaCount: 1

  # Require GPU resources for harmony pods (default: true)
  # WARNING: Setting this to false is only for testing purposes. Harmony requires GPUs for production workloads.
  requireGpu: true

  # Should be equal to, or a divisor of the # of GPUs on each node
  gpusPerReplica: 8

  nodeSelector:   # Uncomment to deploy harmony on specify EKS node group or GKE node pool
    {}
    # eks.amazonaws.com/nodegroup: gpu-node-group-name
    # cloud.google.com/gke-accelerator: a2-ultragpu-4g
  # Adjust to your deployment server specs and model size requirements
  # It is recommended to use as much RAM and CPU as your machine provides
  resources:
    limits:
      cpu: 30
      memory: 500Gi
    requests:
      cpu: 30
      memory: 500Gi

  # Size of /dev/shm (shared memory) volume for NCCL and other IPC
  # This should not be modified under normal circumstances
  shmSize: 32Gi

  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "adaptive"
    prometheus.io/path: /metrics
    prometheus.io/port: "50053"
  podLabels: {}
  extraEnvVars: {}
  # GPU tolerations - only applied when requireGpu is true (default)
  # Set requireGpu: false to disable GPU requirements (testing only)
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  computePool:   # Uncomment to deploy new partitions to handle inference. Any values not specified will be copied from harmony section, otherwise if specified will override them
    # - name: "Inference Pool"
    #   minReplicaCount: 1
    #   maxReplicaCount: 5
    #   tolerations:
    #     - key: "nvidia.com/gpu"
    #       operator: "Exists"
    #       effect: "NoSchedule"
    #   nodeSelector: {}
    #   gpusPerReplica: 4
autoscaling:
  enabled: false    # whether to enable autoscaling for inference (requires prometheus.enabled: true)
  coolDownPeriodSeconds: 180  # duration to await before scaling down pods
  ttftTimeoutThreshold: 0.1   # proportion of timed out completion requests above which the autoscaler decides to scale-out
  # externalPrometheusEndpoint: http://prometheus-server # (Optional) Override the Prometheus endpoint URL if using an external Prometheus instance instead of the built-in one

controlPlane:
  sharedDirType: s3
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: control-plane:latest   # Add the control plane image tag
    pullPolicy: Always

  servicePort: 80   # Port where app will be exposed

  # Full url of the application as visible from a web browser. Important if you use SSO
  rootUrl: "http://localhost:9000"

  # rootUrl: "https://YOUR_URL"

  # Update the DB schema; defaults to True unless explictly False
  runDbMigrations: true

  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "adaptive"
    prometheus.io/path: /metrics
    prometheus.io/port: "9009"
  podLabels: {}

  nodeSelector: {}

  # Uncomment to allow control plane to be scheduled on GPU nodes
  tolerations:
  #   - key: "nvidia.com/gpu"
  #     operator: "Exists"
  #     effect: "NoSchedule"

  extraEnvVars: {}

# Sandboxing service
sandkasten:
  servicePort: 3005
  podAnnotations:
    ingest-adaptive-logs: "true"
  podLabels: {}
  nodeSelector: {}
  tolerations: {}
  extraEnvVars: {}
  replicaCount: 2
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: latest   # Add the sandkasten image tag
    pullPolicy: Always
  resources:
    limits:
      cpu: 2000m
      memory: 32Gi
    requests:
      cpu: 2000m
      memory: 32Gi

# prometheus stack settings
prometheus:
  enabled: true  # Set to false to disable Prometheus subchart (required for autoscaling)
  prometheus-node-exporter:
    enabled: false

  prometheus-pushgateway:
    enabled: false

  kube-state-metrics:
    enabled: false

  alertmanager:
    ## disable alert manager
    enabled: false

  server:
    fullnameOverride: "adaptive-prometheus"

    name: prometheus

    # restrict prometheus scope to the namespace it's installed in
    releaseNamespace: true

    ## Prometheus server container image
    image:
      repository: quay.io/prometheus/prometheus
      # if not set appVersion field from Chart.yaml is used
      tag: "v3.1.0"
      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
      digest: ""
      pullPolicy: IfNotPresent

    replicaCount: 2

    statefulSet:
      enabled: true

    ## Prometheus data retention period
    retention: "30d"

    persistentVolume:
      ## when true, prometheus uses PVC defined using "storageClass"
      enabled: false
      size: 10Gi

    ## storage class
    storageClass: ""

  ## Prometheus scraping configuration (adavanced - do not change this )
  serverFiles:
    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
      ## Below two files are DEPRECATED will be removed from this default values file
        - /etc/config/rules
        - /etc/config/alerts

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
              - localhost:9090

        # A scrape configuration for running Prometheus on a Kubernetes cluster.
        # This uses separate scrape configs for cluster components (i.e. API server, node)
        # and services to allow each to use different authentication configs.
        #
        # Kubernetes labels will be added as Prometheus labels on metrics via the
        # `labelmap` relabeling action.

        # Scrape config for API servers.
        #
        # Kubernetes exposes API servers as endpoints to the default/kubernetes
        # service so this uses `endpoints` role and uses relabelling to only keep
        # the endpoints associated with the default/kubernetes service using the
        # default named port `https`. This works for single API server deployments as
        # well as HA API server deployments.
        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            # insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

        - job_name: 'kubernetes-nodes'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            # insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics


        - job_name: 'kubernetes-nodes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            # insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          # This configuration will work only on kubelet 1.7.3+
          # As the scrape endpoints for cAdvisor have changed
          # if you are using older version you need to change the replacement to
          # replacement: /api/v1/nodes/$1:4194/proxy/metrics
          # more info here https://github.com/coreos/prometheus-operator/issues/633
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

          # Metric relabel configs to apply to samples before ingestion.
          # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
          # metric_relabel_configs:
          # - action: labeldrop
          #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)

        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus-adaptive/scrape`: Only scrape pods that have a value of `true`,
        # except if `prometheus-adaptive/scrape-slow` is set to `true` as well.
        # * `prometheus-adaptive/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus-adaptive/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus-adaptive/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods'
          scrape_interval: 5s
          honor_labels: true

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: adaptive
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              action: replace
              regex: (https?)
              target_label: __scheme__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
              replacement: '[$2]:$1'
              target_label: __address__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);((([0-9]+?)(\.|$)){4})
              replacement: $2:$1
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_phase]
              regex: Pending|Succeeded|Failed|Completed
              action: drop
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node


keda:
  crds:
    # -- Defines whether the KEDA CRDs have to be installed or not.
    install: true

s3proxy:  # set s3proxy.enabled=true for deployments using azure blob storage instead of s3
  enabled: false
  fullnameOverride: "adaptive-s3proxy"
  azure:  # secret names/keys for azure blob storage to proxy
    storageAccount:
      #  -- Azure storage account name
      name: your_azure_account_name
      #  -- Azure access key
      accessKey: your_azure_access_key


tensorboard:
  # If set to true, harmony jobs will use tensorboard for logging
  # A tensorboard sidecar pod will be deployed next to the harmony
  # ordinal 0 pod in the statefulset
  enabled: false
  imageUri: tensorflow/tensorflow:latest
  image:
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 1Gi
  # Configure peristence of the logs,
  # by default use an ephemeral volume, so data will be lost after a restart.
  persistentVolume:
    enabled: false
    annotations: {}
    labels: {}
    accessModes: []
    storageClass: ""
  emptyDir:
    sizeLimit: null

mlflow:
  # If set to true, jobs will use MLflow for experiment tracking
  # A separate MLflow tracking server will be deployed
  # Takes priority over tensorboard if both are enabled
  enabled: true
  # External MLflow configuration
  # Set external.enabled to true to use an existing external MLflow server
  # instead of deploying one within the cluster
  external:
    enabled: false
    # URL of the external MLflow tracking server (e.g., "http://mlflow.example.com:5000")
    url: ""
  imageUri: ghcr.io/mlflow/mlflow:v3.1.1
  image:
    pullPolicy: IfNotPresent

  replicaCount: 1
  workers: 4  # Recommended: 2-4 workers per CPU core. With 1 CPU limit, 4 workers is optimal

  # MLflow server configuration
  backendStoreUri: sqlite:///mlflow-storage/mlflow.db
  # Use mlflow-artifacts:/ URI scheme - artifacts sent via HTTP to server, stored server-side
  # This way, multiple training partitions can upload artifacts without relying on shared storage
  # Highly advised to stick to this scheme
  defaultArtifactRoot: mlflow-artifacts:/
  serveArtifacts: true

  # Storage configuration for mlflow database and artifacts
  volumes:
    - name: mlflow-storage
      emptyDir: {}
    # Example for NFS hostPath:
    # - name: mlflow-storage
    #   hostPath:
    #     path: /mnt/nfs/mlflow
    #     type: Directory

  mountPath: ""
  # Ensure mountPath matches the backendStoreUri
  volumeMounts:
    - name: mlflow-storage
      mountPath: /mlflow-storage

  # Resources for MLflow server
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

  # Node selection and tolerations
  nodeSelector: {}
  tolerations: []

  # Pod annotations and labels
  podAnnotations: {}
  podLabels: {}

  # Additional environment variables
  extraEnvVars: {}


alloy:
  imageUri: grafana/alloy:latest
  image:
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 512Mi

# Optional: This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

redis:  # Redis is required and cannot be disabled
  # OPTIONAL: Set to true to deploy an internal Redis instance in the cluster
  # DEFAULT BEHAVIOR: Deploy internal Redis (set install.enabled=false to use external Redis)
  # When install.enabled=true, the internal Redis service will be used instead of external Redis
  # We recommend using an external managed Redis for production
  install:
    enabled: true  # Set to false to use external Redis endpoint

  # Internal Redis configuration (used when install.enabled=true)
  image:
    repository: redis
    tag: "7.4.7-alpine"
    pullPolicy: IfNotPresent

  port: 6379

  # Redis authentication (optional, only used for internal Redis)
  auth:
    username: ""  # Leave empty for no username
    password: ""  # Leave empty for no password

  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 512Mi

  podAnnotations: {}
  podLabels: {}
  extraEnvVars: {}
  nodeSelector: {}
  tolerations: []

  # External Redis configuration (used when install.enabled=false)
  external:
    # URL of the external Redis server (e.g., "redis://redis.example.com:6379" or "redis://user:pass@redis.example.com:6379")
    url: ""  # Required when install.enabled=false

installPostgres:
  # OPTIONAL: Set to true to deploy an internal PostgreSQL database in the cluster
  # DEFAULT BEHAVIOR: Use external PostgreSQL (configured via secrets.db above)
  # When enabled=true, the internal PostgreSQL service will be used instead of external database
  # We strongly recommend using an external managed database for production
  enabled: false

  image:
    repository: postgres
    tag: "17-alpine"
    pullPolicy: IfNotPresent

  port: 5432

  # Database configuration
  database: adaptive
  username: adaptive
  password: ""  # Leave empty to auto-generate, or set a custom password

  # Set to true for PostgreSQL images with embedded data dumps (testing scenarios)
  # When enabled, no volume is mounted to /var/lib/postgresql/data, allowing the image's
  # built-in data to be used. This also sets PGDATA to /var/lib/postgresql/data (no subdirectory).
  # WARNING: Data will be lost when the pod is deleted. Only use for testing.
  useEmbeddedData: false

  # PostgreSQL persistence
  persistence:
    enabled: false
    storageClass: ""  # Use default storage class if empty
    size: 10Gi
    accessModes:
      - ReadWriteOnce

  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

  podAnnotations: {}
  podLabels: {}
  extraEnvVars: {}
  nodeSelector: {}
  tolerations: []
  # Optional init containers to run before the main postgresql container
  # Example:
  # initContainers:
  #   - name: init-db
  #     image: busybox
  #     command: ['sh', '-c', 'echo "Initializing database..."']
  #     volumeMounts:
  #       - name: init-script
  #         mountPath: /scripts
  initContainers: []
  # Extra volumes to be mounted in the pod (available to init containers and main container)
  # Example:
  # extraVolumes:
  #   - name: init-script
  #     configMap:
  #       name: postgres-init-script
  #   - name: backup-storage
  #     persistentVolumeClaim:
  #       claimName: postgres-backup-pvc
  extraVolumes: []
  # Extra volume mounts for the main postgresql container
  # Example:
  # extraVolumeMounts:
  #   - name: init-script
  #     mountPath: /docker-entrypoint-initdb.d
  #     readOnly: true
  extraVolumeMounts: []

installMinio:
  # OPTIONAL: Set to true to deploy an internal MinIO S3-compatible object storage in the cluster
  # DEFAULT BEHAVIOR: Use external S3 storage (configured via secrets.modelRegistryUrl and secrets.sharedDirectoryUrl above)
  # When enabled=true, the internal MinIO service will be used instead of external S3 storage
  # We strongly recommend using an external managed S3 service for production
  enabled: false

  # MinIO username (default: "adaptive")
  username: adaptive
  # MinIO password (required when enabled=true)
  password: ""

  # Default bucket name for model registry and shared directory
  bucketName: adaptive

  # MinIO persistence
  persistence:
    enabled: false
    storageClass: ""  # Use default storage class if empty
    size: 20Gi
    accessModes:
      - ReadWriteOnce

  # MinIO resources
  resources:
    limits:
      cpu: 2000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 1Gi

  podAnnotations: {}
  podLabels: {}
  nodeSelector: {}
  tolerations: []

# MinIO subchart configuration (Bitnami MinIO chart)
# See https://github.com/bitnami/charts/tree/main/bitnami/minio for full options
# Note: This section is only used when installMinio.enabled=true
# IMPORTANT: When installMinio.enabled=true, you MUST set:
#   - minio.auth.rootUser to match installMinio.username (default: "adaptive")
#   - minio.auth.rootPassword to match installMinio.password (required)
minio:
  # Authentication configuration
  # These values MUST match installMinio.username and installMinio.password when installMinio.enabled=true
  auth:
    rootUser: ""  # REQUIRED when installMinio.enabled=true: Set to installMinio.username (default: "adaptive")
    rootPassword: ""  # REQUIRED when installMinio.enabled=true: Set to installMinio.password
  # Service configuration
  service:
    type: ClusterIP
    ports:
      api: 9000
      console: 9001
  # Persistence configuration
  # NOTE: Helm does NOT automatically inherit values from installMinio.persistence.
  # If you want minio.persistence to match installMinio.persistence, you MUST set both sections to the same values manually.
  persistence:
    enabled: false
    storageClass: ""
    size: 20Gi
    accessModes:
      - ReadWriteOnce
  # Resources
  # NOTE: Helm does NOT automatically inherit values from installMinio.resources.
  # If you want minio.resources to match installMinio.resources, you MUST set both sections to the same values manually.
  resources:
    limits:
      cpu: 2000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 1Gi
  # Node selector and tolerations
  nodeSelector: {}
  tolerations: []
  # Pod annotations and labels
  podAnnotations: {}
  podLabels: {}
  # Default buckets to create
  # Set this to the bucket name from installMinio.bucketName (default: "adaptive")
  # The bucket will be created automatically on MinIO startup
  # If left empty, buckets will be created by the application on first use.
  # WARNING: This requires the application (or MinIO) to have permissions to create buckets.
  # In production environments with restricted IAM policies, ensure buckets are pre-created and specified here,
  # or grant the necessary permissions for bucket creation. Required S3/MinIO permissions: s3:CreateBucket (or equivalent).
  defaultBuckets: ""
  # Additional configuration
  extraEnvVars: []
# Additional volumes/mount for adaptive stack. They don't need to be defined in most of the cases. For cases where model registry is in external nfs mount.
volumes: []
    # - name: model-registry
    #   persistentVolumeClaim:
    #     claimName: model-registry-pvc
volumeMounts: []
    # - name: model-registry
    #   mountPath: /model-registry
# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
