serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: serviceaccount.adaptive-ml.com

secrets:
  # S3 bucket for model registry
  modelRegistryUrl: s3://bucket-name/model_registry

  # Use same bucket as above and can use a different prefix
  sharedDirectoryUrl: s3://bucket-name/shared

  # Postgres database connection string
  dbUrl: postgres://username:password@db_adress:5432/db_name

  # Secret used to sign cookies. Must be the same on all servers of a cluster and >= 64 chars
  cookiesSecret: change-me-secret-db40431e-c2fd-48a6-acd6-854232c2ed94-01dd4d01-dr7b-4315   # Must be >= 64 chars

  auth:
    oidc:
      providers:
        # Name of your OpenId provider displayed in the ui
        - name: "Google"
          # Key of your provider, the callback url will be '<rootUrl>/api/v1/auth/login/<key>/callback'
          key: "google"

          issuer_url: "https://accounts.google.com"   # openid connect issuer url

          client_id: "replace_client_id"   # client id

          client_secret: "replace_client_secret"   # client_secret, optional

          scopes: ["email", "profile"]   # scopes required for auth, requires email and profile

          # true if your provider supports pkce (recommended)
          pkce: true

          # if true, user account will be created if it does not exist
          allow_sign_up: true

auth:
  # One of [admin, read-only, inference, annotator]
  default_role: admin
  session:
    # Set the secure flag for the session cookie: they are only valid on https and localhost
    # Should be true in prod - (use false if the app is accessed through insecure http)
    secure: true
    expiration_seconds: 518400   # 6 days
  # List of email addresses for admins; overrides default_role when these users are created
  admins: []

containerRegistry: <aws_account_id>.dkr.ecr.<region>.amazonaws.com   # Add the Adaptive Registry you have been granted access to

harmony:
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: harmony:latest   # Add the harmony image tag
    pullPolicy: Always

  group: default   # Visible name of the group for this statefulset
  partitionKey: default-statefulset   # Partition name, should be unique across other groups

  replicaCount: 1

  # Should be equal to, or a divisor of the # of GPUs on each node
  # If you are not deploying on GPU, comment out the line
  gpusPerReplica: 8

  nodeSelector:   # Uncomment to deploy harmony on specify EKS node group or GKE node pool
    {}
    # eks.amazonaws.com/nodegroup: gpu-node-group-name
    # cloud.google.com/gke-accelerator: a2-ultragpu-4g
  # Adjust to your deployment server specs and model size requirements
  # It is recommended to use as much RAM and CPU as your machine provides
  resources:
    limits:
      cpu: 30
      memory: 500Gi
    requests:
      cpu: 30
      memory: 500Gi

  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "true"
    prometheus.io/path: /metrics
    prometheus.io/port: 50053
  podLabels: {}
  extraEnvVars: {}

  inferenceFleet: []   # Uncomment to deploy new partitions to handle inference
  #   - name: "Inference Pool"
  #     replicaCount: 0

controlPlane:
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: control-plane:latest   # Add the control plane image tag
    pullPolicy: Always

  servicePort: 80   # Port where app will be exposed

  # Full url of the application as visible from a web browser. Important if you use SSO
  rootUrl: "http://localhost:9000"

  # rootUrl: "https://YOUR_URL"

  # Update the DB schema; defaults to True unless explictly False
  runDbMigrations: true

  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "true"
    prometheus.io/path: /metrics
    prometheus.io/port: 9000
  podLabels: {}

  nodeSelector: {}

  # Uncomment to allow control plane to be scheduled on GPU nodes
  tolerations:
  #   - key: "nvidia.com/gpu"
  #     operator: "Exists"
  #     effect: "NoSchedule"

  extraEnvVars: {}

# prometheus stack settings
prometheus-community:
  prometheus-node-exporter:
    enabled: false

  prometheus-pushgateway:
    enabled: false

  kube-state-metrics:
    enabled: false

  alertmanager:
    ## disable alert manager
    enabled: false

  server:
    name: prometheus

    # restrict prometheus scope to the namespace it's installed in
    releaseNamespace: true

    ## Prometheus server container image
    image:
      repository: quay.io/prometheus/prometheus
      # if not set appVersion field from Chart.yaml is used
      tag: "v3.1.0"
      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
      digest: ""
      pullPolicy: IfNotPresent

    replicaCount: 2

    statefulSet:
      enabled: true

    ## Prometheus data retention period
    retention: "30d"
    persistentVolume:
      ## when true, prometheus uses PVC defined using "storageClass"
      enabled: false
      size: 10Gi

    ## storage class
    storageClass: "gp3"
